{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brain Network Model fitting using RNN\n",
    "     Zheng Wang\n",
    "     \n",
    "     Two type models: Linear Model and Wong-Wang-Deco Model\n",
    "     \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoRNN():\n",
    "    \n",
    "    param = {\n",
    "    \n",
    "    # Parameters for the integration\n",
    "    \n",
    "    \"ROI_num\" : 96,       # number of neural nodes\n",
    "    \"sigma\"      : 0.02,    # standard deviation of the Gaussian noise\n",
    "    \n",
    "    # Parameters for the ODEs\n",
    "    # Excitatory population\n",
    "    \"WE\" : 1.,              # scale of the external input\n",
    "    \"tauE\" : 100.,          # decay time\n",
    "    \"gamma_E\" : 0.641/1000.,       # other dynamic parameter (?)\n",
    "    \n",
    "    # Inhibitory population\n",
    "    \"WI\" : 0.7,             # scale of the external input\n",
    "    \"tauI\" : 10.,           # decay time\n",
    "    \"gamma_I\" : 1./1000.,          # other dynamic parameter (?)\n",
    "    \n",
    "    # External input\n",
    "    \"I0\" : 0.5, # 0.32,          # external input\n",
    "    \"Ie\" : 0.,       # external stimulation\n",
    "    \n",
    "    # Coupling parameters\n",
    "    \"g\" : 100.,               # global coupling (from all nodes E_j to single node E_i)\n",
    "    \"gEE\" : .1,            # local self excitatory feedback (from E_i to E_i)\n",
    "    \"gIE\" : .1,            # local inhibitory coupling (from I_i to E_i)\n",
    "    \"gEI\" : 0.1,            # local excitatory coupling (from E_i to I_i)\n",
    "    \"lamb\" : 0.,             # scale of global coupling on I_i (compared to E_i)\n",
    "       \n",
    "    \"aE\":310,\n",
    "    \"bE\" :125,\n",
    "    \"dE\":0.16,\n",
    "    \"aI\":615,\n",
    "    \"bI\" :177, \n",
    "    \"dI\" :0.087, \n",
    "       \n",
    "    # Output (BOLD signal)\n",
    "   \n",
    "    \"alpha\" : 0.32,\n",
    "    \"rho\" : 0.34,\n",
    "    \"k1\" : 2.38,\n",
    "    \"k2\" : 2.0,\n",
    "    \"k3\" : 0.48, # adjust this number from 0.48 for BOLD fluctruate around zero\n",
    "    \"V\" : .02,\n",
    "    \"E0\" : 0.34, \n",
    "    \"tau_s\" : 0.65,\n",
    "    \"tau_f\" : 0.41,\n",
    "    \"tau_0\" : 0.98\n",
    "   \n",
    "    } ### initial values of all model prameters\n",
    "    \n",
    "    \n",
    "    def __init__(self, num_nodes):\n",
    "        \n",
    "        self.param['ROI_num'] = num_nodes\n",
    "        self.L= tf.zeros((num_nodes,num_nodes))\n",
    "        self.num_states = 6\n",
    "        self.num_states_noise = 2\n",
    "        self.variables_name = ['E', 'I', 'x', 'f', 'v', 'q']\n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "    def dfun(self, X):\n",
    "        \n",
    "        E=X[:,0:1]\n",
    "        I=X[:,1:2]\n",
    "        x=X[:,2:3]\n",
    "        f=X[:,3:4]\n",
    "        v=X[:,4:5]\n",
    "        q=X[:,5:6]\n",
    "        \n",
    "        \n",
    "       \n",
    "        def fout(z, alpha):\n",
    "            return tf.pow(z, 1./alpha)\n",
    "        def Ef(rho,z):\n",
    "        \n",
    "            return 1.0-tf.pow(1.-rho, 1./z)\n",
    "        def h_tf(a, b, d, z):\n",
    "            return (0.00001+tf.abs(a*z-b))/(0.00001*d+tf.abs(1.0000 -tf.exp(-d*(a*z-b))))\n",
    "\n",
    "       \n",
    "        IE = tf.nn.relu(self.param['WE']*self.param['I0'] + self.param['gEE']*E + self.param['g']*tf.matmul(self.L, E)\\\n",
    "                        -self.param['gIE']*(I) + self.param['Ie'])\n",
    "        II = tf.nn.relu(self.param['WI']*self.param['I0'] + self.param['gEI']*E - I)\n",
    "        \n",
    "        #IE = 125/310.0 -0.026 +0.006*tf.tanh(IE/100. - 125/310.0 + 0.026) \n",
    "        #(3.0631+0.4869*tf.tanh(h_tf(aE, bE, dE, IE)-3.0631)) \n",
    "        E_new = -E/self.param['tauE'] +(1.0 -E)*self.param['gamma_E']*h_tf(self.param['aE'], self.param['bE'], \\\n",
    "                                                                           self.param['dE'], IE)\n",
    "        I_new = - I/self.param['tauI'] +self.param['gamma_I']*h_tf(self.param['aI'], self.param['bI'], self.param['dI'], II) \n",
    "    \n",
    "    \n",
    "        dx = E - I -1.0/self.param['tau_s']*x  -1.0/self.param['tau_f']*(f-1.0)\n",
    "        df = x \n",
    "        dv = (f -v**(1./self.param['alpha']))/self.param['tau_0'] #f/self.param['tau_0'] -fout(v, self.param['alpha'])/self.param['tau_0']\n",
    "        dq = (f*(1.-(1.-self.param['rho'])**(1./f))/self.param['rho'] -q*(v)**(1./self.param['alpha'])/v)/self.param['tau_0']\n",
    "        #f*Ef(self.param['rho'],f)/self.param['rho']/self.param['tau_0']-q*fout(v, self.param['alpha'])/v/self.param['tau_0']\n",
    "        \n",
    "        return tf.concat([E_new, I_new, dx, df, dv, dq], axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cost_fun():\n",
    "    def __init__(self, logits_series, labels_series, batch_size):\n",
    "        if len(logits_series) == len(labels_series):\n",
    "            for i in range(len(logits_series)):\n",
    "                if not logits_series[0].shape == labels_series[0].shape:\n",
    "                    print('not matching')\n",
    "                    break\n",
    "            \n",
    "            self.logits_series = logits_series\n",
    "            self.labels_series = labels_series\n",
    "            self.batch_size = batch_size\n",
    "     \n",
    "\n",
    "    def cost_r(self):\n",
    "        \n",
    "        labels_series_tf = tf.stack(self.labels_series,axis=1)\n",
    "        logits_series_tf = tf.stack(self.logits_series,axis=1)\n",
    "\n",
    "        labels_series_tf_n = labels_series_tf - tf.matmul(tf.reshape(tf.reduce_mean(labels_series_tf, 1), [self.batch_size,1]),\\\n",
    "                            tf.constant(np.ones((1,truncated_backprop_length)), dtype=tf.float32))\n",
    "        logits_series_tf_n = logits_series_tf - tf.matmul(tf.reshape(tf.reduce_mean(logits_series_tf, 1), [self.batch_size,1]),\\\n",
    "                            tf.constant(np.ones((1,truncated_backprop_length)), dtype=tf.float32))\n",
    "\n",
    "\n",
    "        cov_sim =tf.matmul(logits_series_tf_n, tf.transpose(logits_series_tf_n))\n",
    "        cov_def= tf.matmul(labels_series_tf_n, tf.transpose(labels_series_tf_n))\n",
    "\n",
    "\n",
    "        FC_sim_T = tf.matmul(tf.matmul(tf.diag(tf.reciprocal(tf.sqrt(tf.diag_part(cov_sim)))), cov_sim), \\\n",
    "                     tf.diag(tf.reciprocal(tf.sqrt(tf.diag_part(cov_sim)))))\n",
    "        FC_T = tf.matmul(tf.matmul(tf.diag(tf.reciprocal(tf.sqrt(tf.diag_part(cov_def)))), cov_def), \\\n",
    "                 tf.diag(tf.reciprocal(tf.sqrt(tf.diag_part(cov_def))))) \n",
    "        ones_tri=tf.matrix_band_part(tf.ones_like(FC_T)-tf.diag(tf.ones((self.batch_size,))), 0, -1)\n",
    "        zeros = tf.zeros_like(FC_T) # create a tensor all ones\n",
    "        mask = tf.greater(ones_tri, zeros) # boolean tensor, mask[i] = True iff x[i] > 1\n",
    "        FC_tri_v = tf.boolean_mask(FC_T, mask)\n",
    "\n",
    "        FC_v = FC_tri_v - tf.reduce_mean(FC_tri_v)*tf.ones_like(FC_tri_v)\n",
    "\n",
    "\n",
    "        FC_sim_tri_v = tf.boolean_mask(FC_sim_T, mask)\n",
    "        FC_sim_v = FC_sim_tri_v - tf.reduce_mean(FC_sim_tri_v)*tf.ones_like(FC_sim_tri_v)\n",
    "\n",
    "        corr_FC =tf.reduce_sum(tf.multiply(FC_v,FC_sim_v))\\\n",
    "                  /tf.sqrt(tf.reduce_sum(tf.multiply(FC_v,FC_v)))\\\n",
    "                /tf.sqrt(tf.reduce_sum(tf.multiply(FC_sim_v,FC_sim_v)))\n",
    "        \n",
    "        losses_corr = tf.square(1- corr_FC)\n",
    "        losses = tf.sqrt(tf.reduce_mean(tf.multiply(FC_sim_tri_v-FC_tri_v, FC_sim_tri_v-FC_tri_v)))\n",
    "        return losses\n",
    "    def cost_dist(self):\n",
    "        \n",
    "        labels_series_tf = tf.stack(self.labels_series,axis=1)\n",
    "        logits_series_tf = tf.stack(self.logits_series,axis=1)\n",
    "\n",
    "        labels_series_tf_n = labels_series_tf - tf.matmul(tf.reshape(tf.reduce_mean(labels_series_tf, 1), [self.batch_size,1]),\\\n",
    "                            tf.constant(np.ones((1,truncated_backprop_length)), dtype=tf.float32))\n",
    "        logits_series_tf_n = logits_series_tf - tf.matmul(tf.reshape(tf.reduce_mean(logits_series_tf, 1), [self.batch_size,1]),\\\n",
    "                            tf.constant(np.ones((1,truncated_backprop_length)), dtype=tf.float32))\n",
    "\n",
    "\n",
    "        cov_sim =tf.matmul(logits_series_tf_n, tf.transpose(logits_series_tf_n))\n",
    "        cov_def= tf.matmul(labels_series_tf_n, tf.transpose(labels_series_tf_n))\n",
    "\n",
    "\n",
    "        FC_sim_T = tf.matmul(tf.matmul(tf.diag(tf.reciprocal(tf.sqrt(tf.diag_part(cov_sim)))), cov_sim), \\\n",
    "                     tf.diag(tf.reciprocal(tf.sqrt(tf.diag_part(cov_sim)))))\n",
    "        FC_T = tf.matmul(tf.matmul(tf.diag(tf.reciprocal(tf.sqrt(tf.diag_part(cov_def)))), cov_def), \\\n",
    "                 tf.diag(tf.reciprocal(tf.sqrt(tf.diag_part(cov_def))))) \n",
    "        ones_tri=tf.matrix_band_part(tf.ones_like(FC_T)-tf.diag(tf.ones((self.batch_size,))), 0, -1)\n",
    "        zeros = tf.zeros_like(FC_T) # create a tensor all ones\n",
    "        mask = tf.greater(ones_tri, zeros) # boolean tensor, mask[i] = True iff x[i] > 1\n",
    "        FC_tri_v = tf.boolean_mask(FC_T, mask)\n",
    "\n",
    "        FC_v = FC_tri_v - tf.reduce_mean(FC_tri_v)*tf.ones_like(FC_tri_v)\n",
    "\n",
    "\n",
    "        FC_sim_tri_v = tf.boolean_mask(FC_sim_T, mask)\n",
    "        FC_sim_v = FC_sim_tri_v - tf.reduce_mean(FC_sim_tri_v)*tf.ones_like(FC_sim_tri_v)\n",
    "\n",
    "        losses = tf.sqrt(tf.reduce_mean(tf.multiply(FC_sim_v-FC_v, FC_sim_v-FC_v)))\n",
    "        \n",
    "        losses_dist = tf.reduce_mean(losses)\n",
    "        return losses_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_graph():\n",
    "    \n",
    "    ### default fit model parameteres:\n",
    "    \n",
    "    fit_param = ['g', 'gEE', 'gIE', 'gEI', 'sigma']\n",
    "    \n",
    "        \n",
    "    def __init__(self, step_size, Tr, truncated_backprop_length, f,  fit_conecctiongains):\n",
    "        #self.args = kwargs\n",
    "       \n",
    "        def smooth_normalize_ct(x, center):\n",
    "            x_n = center+ (center-0.0001)*tf.tanh((x-center)/(center - 0.0001))\n",
    "            return x_n\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        self.fit_cg = fit_conecctiongains\n",
    "        \n",
    "        self.dt = step_size\n",
    "        self.truncated_backprop_length = truncated_backprop_length\n",
    "        self.Tr = Tr\n",
    "        self.hidden_num = np.int(Tr/step_size)\n",
    "        self.num_states = f.num_states\n",
    "        self.num_states_noise = f.num_states_noise\n",
    "        self.batch_size = f.param['ROI_num']\n",
    "        self.num_inputs = 1+self.truncated_backprop_length*(1+self.num_states_noise*self.hidden_num)\n",
    "        \n",
    "        self.adjacency = tf.placeholder(tf.float32, [self.batch_size, self.batch_size])\n",
    "        self.batchX_placeholder = tf.placeholder(tf.float32, [self.batch_size, self.num_inputs])\n",
    "        self.batchY_placeholder = tf.placeholder(tf.float32, [self.batch_size, self.truncated_backprop_length])\n",
    "\n",
    "        f.L = -tf.diag(tf.reduce_sum(self.adjacency, axis =1)) + self.adjacency\n",
    "        #L = -tf.diag(tf.reduce_sum(self.adjacency, axis =1)) + self.adjacency\n",
    "        self.f = f\n",
    "        \n",
    "        self.init_paras = [80, 0.5, 0.5, 1.0, 0.02]\n",
    "        \n",
    "       \n",
    "        variables_ls =[]\n",
    "        \n",
    "        for para in range(len(self.fit_param)):\n",
    "            \n",
    "            self.f.param[self.fit_param[para]] = tf.Variable(self.init_paras[para], dtype=tf.float32)\n",
    "            variables_ls.append(self.f.param[self.fit_param[para]])\n",
    "            \n",
    "        self.variables_ls = tuple(variables_ls)\n",
    "        self.Ws = tf.Variable(0.05+np.zeros((self.f.param['ROI_num'],self.f.param['ROI_num'])), dtype=tf.float32)\n",
    "        \n",
    "        if self.fit_cg == True:\n",
    "            \n",
    "            W_n= (self.Ws+tf.transpose(self.Ws))/2.0\n",
    "            W_n = tf.exp(W_n)*self.adjacency#W_n - tf.diag(tf.diag_part(W_n))#tf.exp(2.0*W_n) * L_new# (tf.ones((batch_size, batch_size))- tf.diag(tf.diag_part(tf.ones((batch_size, batch_size)))))#\n",
    "\n",
    "            W_s =W_n/tf.norm(W_n)\n",
    "            self.f.L= -tf.diag(tf.reduce_sum(W_s, axis=1)) + W_s\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        for para in self.fit_param:\n",
    "            if para != 'sigma':\n",
    "                self.f.param[para] = 0.001+tf.nn.relu(self.f.param[para])\n",
    "            else:\n",
    "                self.f.param[para] = 0.01+tf.nn.relu(self.f.param[para])\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "                \n",
    "        self.init_state = tf.placeholder(tf.float32, [self.batch_size, self.num_states])\n",
    "        \n",
    "        \n",
    "        # Unpack columns\n",
    "\n",
    "        inputs_series = tf.unstack(self.batchX_placeholder, axis=1)\n",
    "        labels_series = tf.unstack(self.batchY_placeholder, axis=1)\n",
    "        print(len(inputs_series))\n",
    "        \n",
    "        current_state =  self.init_state\n",
    "        states_series = []\n",
    "        for j in range(self.truncated_backprop_length):\n",
    "            #print(j)\n",
    "            for i in range(self.hidden_num):\n",
    "                #print(i)\n",
    "                noises=[]\n",
    "                for s in range(self.num_states_noise):\n",
    "                    noises.append(inputs_series[s*self.truncated_backprop_length*self.hidden_num\\\n",
    "                                          +j*self.hidden_num+i])\n",
    "                \n",
    "                current_state = current_state + self.dt*self.f.dfun(current_state)\n",
    "                #print('next',next_state.shape)\n",
    "                current_states_series = tf.unstack(current_state, axis = 1)\n",
    "                current_states_series[0] = tf.tanh(current_states_series[0] \\\n",
    "                              +tf.sqrt(self.dt)*self.f.param['sigma']*noises[0])\n",
    "                current_states_series[1] = tf.tanh(current_states_series[1] \\\n",
    "                              +tf.sqrt(self.dt)*self.f.param['sigma']*noises[1])\n",
    "                current_states_series[2] = tf.tanh(current_states_series[2])\n",
    "                \n",
    "                current_states_series[3] = smooth_normalize_ct(current_states_series[3],1.0)# + tf.tanh(current_states_series[3]-1.0)\n",
    "                current_states_series[4] = smooth_normalize_ct(current_states_series[4],1.0)#1.0 + tf.tanh(current_states_series[4]-1.0)\n",
    "                current_states_series[5] = smooth_normalize_ct(current_states_series[5],1.0)#1.0 + tf.tanh(current_states_series[5]-1.0)\n",
    "                \n",
    "                \n",
    "                current_state = tf.stack(current_states_series, axis = 1)\n",
    "                #print(current_states.shape)\n",
    "            #next_state = E_new\n",
    "            states_series.append(current_state)# Broadcasted addition\n",
    "    \n",
    "            #current_state = next_state \n",
    "        \n",
    "        noise2 = inputs_series[-truncated_backprop_length-1:-1]\n",
    "        \n",
    "        logits_series = [50/0.34*0.02*(self.f.param['k1']*(1.0 -state[:, 5])\\\n",
    "                 +self.f.param['k2']*(1.0-state[:, 5]/state[:, 4])\\\n",
    "                 +self.f.param['k3']*(1.0-state[:, 4]))\\\n",
    "                 +0.02*noise   for state, noise in zip(states_series, noise2)]\n",
    "\n",
    "        print(logits_series[0].shape)\n",
    "        print(labels_series[0].shape)\n",
    "        self.logits_series=logits_series\n",
    "        self.states_series=states_series\n",
    "        self.current_state = current_state\n",
    "        cost = Cost_fun(logits_series, labels_series, self.f.param['ROI_num'])\n",
    "        self.cost = cost\n",
    "   \n",
    "    \n",
    "        total_loss = self.cost.cost_dist() #+ 0.1*(self.f.param['gEE'] + self.f.param['gIE'] -0.3)**2 + 0.1*(self.f.param['gEI'] -1.)**2\n",
    "        \n",
    "\n",
    "\n",
    "        opt_func = tf.train.AdamOptimizer(.01)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        train_step=opt_func.minimize(total_loss)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.train_step = train_step\n",
    "        self.total_loss = total_loss\n",
    "    \n",
    "       \n",
    "   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "from six.moves import cPickle\n",
    "\n",
    "\n",
    "\"\"\"parser = argparse.ArgumentParser(\n",
    "                    formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "# Data and model checkpoints directories\n",
    "parser.add_argument('data_dir=', type=str, default='/brunhild/mcintosh_lab/jwang/results/PPMI/',\n",
    "                    help='data directory containing all subjects')\n",
    "parser.add_argument('save_dir=', type=str, default='save',\n",
    "                    help='directory to store checkpointed models')\n",
    "parser.add_argument('log_dir=', type=str, default='logs',\n",
    "                    help='directory to store tensorboard logs')\n",
    "parser.add_argument('save_every=', type=int, default=1000,\n",
    "                    help='Save frequency. Number of passes between checkpoints of the model.')\n",
    "parser.add_argument('init_from=', type=str, default=None,\n",
    "                    help=continue training from saved model at this path (usually \"save\").\n",
    "                        Path must contain files saved by previous training process:\n",
    "                        'config.pkl'        : configuration;\n",
    "                        \n",
    "                        'checkpoint'        : paths to model file(s) (created by tf).\n",
    "                                              Note: this file contains absolute paths, be careful when moving files around;\n",
    "                        'model.ckpt-*'      : file(s) with model definition (created by tf)\n",
    "                         Model params must be the same between multiple runs (model, rnn_size, num_layers and seq_length))\n",
    "\n",
    "# Optimization\n",
    "parser.add_argument('truncated_backprop_length=', type=int, default=15,\n",
    "                    help='RNN sequence length. Number of timesteps to unroll for.')\n",
    "parser.add_argument('batch_size=', type=int, default=96,\n",
    "                    help=node number)\n",
    "\n",
    "parser.add_argument('Tr=', type=int, default=.05,\n",
    "                    help='Tr fmri')\n",
    "parser.add_argument('step_size=', type=int, default=.001,\n",
    "                    help='Tr fmri')\n",
    "\n",
    "parser.add_argument('step_size=', type=int, default=.001,\n",
    "                    help='Integration step')\n",
    "args = parser.parse_args()\n",
    "\n",
    "import tensorflow as tf\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Brainmodelfitting( ):\n",
    "    \n",
    "    \n",
    "    def __init__(self, model_graph, data_set, echo_step, num_epochs, out_dir):\n",
    "        \n",
    "        self.model = model_graph\n",
    "        self.echo_step = echo_step\n",
    "        self.Tr = model_graph.Tr\n",
    "        self.step_size = model_graph.dt\n",
    "        self.truncated_backprop_length = model_graph.truncated_backprop_length\n",
    "        self.batch_size = model_graph.batch_size\n",
    "        self.data_dir = data_dir\n",
    "        self.out_dir = out_dir\n",
    "        self.num_epochs = num_epochs\n",
    "        \n",
    "        self.data_set = data_set\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    def plot(self, batch_idx, y_array, E_array, I_array, x_array, v_array, f_array, q_array, params_list, loss_list, batchX, batchY):\n",
    "       \n",
    "        plt.subplot(4, 3, 1)\n",
    "        plt.cla()\n",
    "        plt.plot(loss_list)\n",
    "    \n",
    "        plt.subplot(4, 3, 2)\n",
    "        plt.cla()\n",
    "        plt.plot(np.array(params_list)[:,0])\n",
    "    \n",
    "        plt.subplot(4, 3, 4)\n",
    "        plt.cla()\n",
    "        plt.plot(np.array(params_list)[:,5:])\n",
    "        \n",
    "        plt.subplot(4, 3, 3)\n",
    "        plt.cla()\n",
    "        plt.plot(y_array.T)\n",
    "        \n",
    "        plt.subplot(4, 3, 5)\n",
    "        plt.cla()\n",
    "        plt.plot(E_array.T)\n",
    "        \n",
    "        plt.subplot(4, 3, 6)\n",
    "        plt.cla()\n",
    "        plt.plot(I_array.T)\n",
    "        \n",
    "        plt.subplot(4, 3, 7)\n",
    "        plt.cla()\n",
    "        plt.plot(x_array.T)\n",
    "        plt.subplot(4, 3, 8)\n",
    "        plt.cla()\n",
    "        plt.plot(v_array.T)\n",
    "        plt.subplot(4, 3, 9)\n",
    "        plt.cla()\n",
    "        plt.plot(f_array.T)\n",
    "        plt.subplot(4, 3, 10)\n",
    "        plt.cla()\n",
    "        plt.plot(q_array.T)\n",
    "        \n",
    "        plt.subplot(4, 3, 11)\n",
    "        plt.cla()\n",
    "        plt.plot(np.array(params_list)[:,1:2])\n",
    "        \n",
    "        plt.subplot(4, 3, 12)\n",
    "        plt.cla()\n",
    "        plt.plot(np.array(params_list)[:,3])\n",
    "        \n",
    "\n",
    "        plt.draw()\n",
    "        plt.pause(0.0001)\n",
    "    \n",
    "    \n",
    "    def train(self, subID, SC, TS):\n",
    "        \n",
    "        TS_len = TS.shape[0] - self.echo_step\n",
    "        num_batches = TS_len // self.truncated_backprop_length\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            \n",
    "            \n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            saver = tf.train.Saver(tf.global_variables())\n",
    "            \n",
    "            plt.ion()\n",
    "            plt.figure()\n",
    "            plt.show()\n",
    "        \n",
    "            loss_list = []\n",
    "            params_list=[]\n",
    "    \n",
    "   \n",
    "             \n",
    "    \n",
    "            for epoch_idx in range(self.num_epochs):\n",
    "        \n",
    "                y_in= TS.T[:,echo_step:TS_len]/np.max(TS)\n",
    "                \n",
    "                y_array =np.zeros((self.batch_size,num_batches*self.truncated_backprop_length))\n",
    "                E_array =np.zeros((self.batch_size,num_batches*self.truncated_backprop_length))\n",
    "                I_array =np.zeros((self.batch_size,num_batches*self.truncated_backprop_length))\n",
    "                x_array =np.zeros((self.batch_size,num_batches*self.truncated_backprop_length))\n",
    "                v_array =np.zeros((self.batch_size,num_batches*self.truncated_backprop_length))\n",
    "                f_array =np.zeros((self.batch_size,num_batches*self.truncated_backprop_length))\n",
    "                q_array =np.zeros((self.batch_size,num_batches*self.truncated_backprop_length))\n",
    "                initi_con = .45*(np.random.uniform(0,1,[self.batch_size, self.model.num_states])+np.array([[0, 0, 0,.5,.5,.5]]))\n",
    "                _current_state = initi_con\n",
    "                \n",
    "                for batch_idx in range(num_batches):\n",
    "                \n",
    "                    start_idx = batch_idx * self.truncated_backprop_length\n",
    "                    end_idx = start_idx + self.truncated_backprop_length\n",
    "\n",
    "                    batchX = np.random.randn(self.batch_size, 1+self.truncated_backprop_length +\\\n",
    "                                    self.model.num_states_noise*self.truncated_backprop_length* \\\n",
    "                                    self.model.hidden_num)\n",
    "                    batchY = y_in[:,start_idx:end_idx]\n",
    "                    #print(self.model.variables_scalar)\n",
    "\n",
    "                    _variables_s,  _Wg, _current_state, _total_loss, _train_step,\\\n",
    "                               _logits_series, _states_series,  = sess.run(\n",
    "                             [self.model.variables_ls, self.model.Ws, self.model.current_state, \\\n",
    "                              self.model.total_loss, self.model.train_step, self.model.logits_series, self.model.states_series],\n",
    "                        feed_dict={\n",
    "                            self.model.batchX_placeholder:batchX,\n",
    "                            self.model.batchY_placeholder:batchY,\n",
    "                            self.model.init_state:_current_state,\n",
    "                            self.model.adjacency:SC\n",
    "                            })\n",
    "\n",
    "                    params_new = []\n",
    "                    loss_list.append(_total_loss)\n",
    "                    for par_s in _variables_s:\n",
    "                        params_new.append(par_s)\n",
    "                            \n",
    "                    if self.model.fit_cg == True:\n",
    "                        params_new.extend(list(_Wg.ravel()))\n",
    "                    params_list.append(np.array(params_new))\n",
    "                \n",
    "                    for i in range(self.truncated_backprop_length):\n",
    "                        start_idx = batch_idx * self.truncated_backprop_length\n",
    "                \n",
    "                        y_array[:, start_idx+i] = _logits_series[i]\n",
    "                        E_array[:, start_idx+i] = _states_series[i][:,0]\n",
    "                        I_array[:, start_idx+i] = _states_series[i][:,1]\n",
    "                        x_array[:, start_idx+i] = _states_series[i][:,2]\n",
    "                        f_array[:, start_idx+i] = _states_series[i][:,3]\n",
    "                        v_array[:, start_idx+i] = _states_series[i][:,4]\n",
    "                        q_array[:, start_idx+i] = _states_series[i][:,5]\n",
    "                    \n",
    "                            \n",
    "                    \n",
    "                \n",
    "            saver.save(sess, self.out_dir+subID+ 'model.checkpoint')         \n",
    "        \n",
    "            #print( np.array(params_list))\n",
    "            \n",
    "            self.plot(batch_idx, y_array, E_array, I_array, x_array, v_array, f_array, q_array, params_list, loss_list,  batchX, batchY)        \n",
    "            plt.ioff()\n",
    "            plt.show()\n",
    "            np.savetxt(self.out_dir+subID+ 'paramsList.txt', np.array(params_list))\n",
    "            np.savetxt(self.out_dir+subID+ 'sim_fitting_bold.txt', y_array.T)\n",
    "            #y_dmean =(y_array.T -y_array.T.mean(axis= 0)).T\n",
    "            FC_sim = np.corrcoef(y_array[:,10:])\n",
    "    \n",
    "            FC = np.corrcoef(TS.T)\n",
    "            corr_simfit= np.corrcoef(FC[np.tril_indices(self.batch_size,-1)], FC_sim[np.tril_indices(self.batch_size,-1)])[0,1]\n",
    "            print(corr_simfit)\n",
    "        \n",
    "            fig, ax = plt.subplots(1,3, figsize=(20,4))\n",
    "            ax[0].plot(TS)\n",
    "            img1 =ax[1].imshow(FC_sim -np.diag(np.diag(FC_sim)), cmap='bwr')\n",
    "            plt.colorbar(img1, ax=ax[1], fraction=0.046, pad=0.04)\n",
    "            img2 =ax[2].imshow(FC, cmap='bwr')\n",
    "            plt.colorbar(img2, ax=ax[2], fraction=0.046, pad=0.04)\n",
    "            plt.show()\n",
    "                       \n",
    "            return corr_simfit\n",
    "        \n",
    "        \n",
    "                       \n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "                \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='/brunhild/mcintosh_lab/jwang/ModelFitting/TestData/'\n",
    "out_dir='/brunhild/mcintosh_lab/jwang/ModelFitting/test/'\n",
    "data_set='TestData'\n",
    "#model_name='Linear'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1516\n",
      "(96,)\n",
      "(96,)\n"
     ]
    }
   ],
   "source": [
    "groups =[grp for grp in os.listdir(data_dir)]\n",
    "\n",
    "f = DecoRNN(96)\n",
    "batch_size = 96\n",
    "step_size = 0.05\n",
    "Tr= 2.5\n",
    "truncated_backprop_length = 15\n",
    "\n",
    "num_epochs = 300\n",
    "echo_step = 0\n",
    "\n",
    "#graph_lin = Model_graph(step_size, Tr, truncated_backprop_length, f, paras_lin, cost_name)\n",
    "graph_deco = Model_graph(step_size, Tr, truncated_backprop_length, f, True)\n",
    "#F = Brainmodelfitting(graph_lin, data_set,echo_step, num_epochs, out_dir)\n",
    "F = Brainmodelfitting(graph_deco, data_set,echo_step, num_epochs, out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups =['CON_MCI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CON_MCI\n",
      "0 4032\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "para_corr={}\n",
    "for grp in groups:\n",
    "    print(grp)\n",
    "    grp_dir = data_dir +grp + '/'\n",
    "    subs = [sub for sub in os.listdir(grp_dir) if str.isdigit(sub)]\n",
    "    for i in range(len(subs)):\n",
    "        subID =subs[i]\n",
    "            \n",
    "        file_path= '/brunhild/mcintosh_lab/jwang/ModelFitting/' + data_set +'/'\n",
    "        if not os.path.exists(file_path):\n",
    "            os.mkdir(file_path)\n",
    "        print(i, subID)\n",
    "        SC_file = grp_dir + subID + '/preprocess/connectivity/SC/SC.txt'\n",
    "        TS_file = grp_dir + subID + '/preprocess/connectivity/FC/TS.txt'\n",
    "        if os.path.isfile(SC_file) and os.path.isfile(TS_file):\n",
    "            para_corr[data_set+subID]= []\n",
    "            SC= np.loadtxt(SC_file)\n",
    "            TS= np.loadtxt(TS_file)\n",
    "            TS_dmean =(TS.T -TS.T.mean(axis= 0)).T\n",
    "            SC = (SC+SC.T)*0.5\n",
    "            \"\"\"SC1=SC[:batch_size//2,:batch_size//2].copy()\n",
    "            SC2=SC[batch_size//2:batch_size,batch_size//2:batch_size].copy()\n",
    "            SC3=SC[:batch_size//2,batch_size//2:batch_size].copy()\n",
    "            mask1 = (SC1-SC1.mean(axis=1)< 2.*SC1.std(axis=1)) \n",
    "            SC1[mask1]=0\n",
    "            SC2[(SC2-SC2.mean(axis=1) <  2*SC2.std(axis=1)) ]=0\n",
    "            SC3[(SC3-SC3.mean(axis=1)<  2*SC3.std(axis=1)) ]=0\n",
    "            SC[:batch_size//2,:batch_size//2] = SC1\n",
    "            SC[batch_size//2:batch_size,batch_size//2:batch_size] = SC2\n",
    "            SC[:batch_size//2,batch_size//2:batch_size] = 1*SC3\n",
    "            SC[batch_size//2:batch_size,:batch_size//2] = 1*SC3.T\"\"\"\n",
    "            Wo = np.log1p(SC)/np.linalg.norm(np.log1p(SC))\n",
    "                #L_s = (-np.diag(np.sum(W0, axis= 1)) + W0).astype(np.float32)\n",
    "                \n",
    "            corr_fit = F.train(subID, Wo, TS_dmean)\n",
    "            \"\"\"para_corr[data_set+subID].append(corr_fit)\n",
    "            corr_sim =F.test(subID, Wo, TS_dmean)\n",
    "            para_corr[data_set+subID].append(corr_sim)\n",
    "            params = np.loadtxt(out_dir+subID+'paramsList.txt')\n",
    "            Theta = list(params[-10:,:].mean(axis = 0))\n",
    "            para_corr[data_set+subID].extend(Theta)\"\"\"\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DecoRNN at 0x7f128b5de8d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_5:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.param['sigma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add:0' shape=(96, 96) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Variable:0' shape=() dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_1:0' shape=() dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_2:0' shape=() dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_3:0' shape=() dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_4:0' shape=() dtype=float32_ref>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_deco.variables_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
